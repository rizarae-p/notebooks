{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d92080",
   "metadata": {},
   "source": [
    "# Creating a DeepLabCut Multi-Animal PyTorch Project\n",
    "\n",
    "- Copy a project to a new folder\n",
    "- Increase the iteration in the config file\n",
    "- set `default_net_type: dekr_w32` in config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37142aee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11623415",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e1fe21",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deeplabcut'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mA\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdeeplabcut\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeeplabcut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpose_estimation_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     analyze_videos,\n\u001b[1;32m      9\u001b[0m     convert_detections2tracklets,\n\u001b[1;32m     10\u001b[0m     train_network,\n\u001b[1;32m     11\u001b[0m     inference_network,\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deeplabcut'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import deeplabcut\n",
    "from deeplabcut.pose_estimation_pytorch import (\n",
    "    analyze_videos,\n",
    "    convert_detections2tracklets,\n",
    "    train_network,\n",
    "    inference_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620661f",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658a1a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/niels/Documents/upamathis/datasets\"\n",
    "project = f\"{root}/dev-pytorch-ma-pen-2023-06-14\"\n",
    "config_path = f\"{project}/config.yaml\"\n",
    "\n",
    "model_prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bfa3c",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0b7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(path):\n",
    "    try :\n",
    "        with open(path, \"r\") as stream:\n",
    "            try:\n",
    "                return yaml.safe_load(stream)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "    except :\n",
    "        raise FileNotFoundError(\"An eero occured whilereading the file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ff3fa",
   "metadata": {},
   "source": [
    "## Create the PyTorch Training Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903e6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing the following graph: [[0, 1]]\n",
      "Creating training data for: Shuffle: 1 TrainFraction:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 2404.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n",
      "Creating training data for: Shuffle: 2 TrainFraction:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 13586.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shuffles = deeplabcut.create_training_dataset(\n",
    "    config_path,\n",
    "    num_shuffles=2,\n",
    "    net_type=\"dekr_w18\",\n",
    ")\n",
    "\n",
    "shuffles = None\n",
    "if shuffles is not None:\n",
    "    for shuffle in shuffles:\n",
    "        print(80 * \"-\")\n",
    "        print(f\"split = {shuffle[0]}\")\n",
    "        print(f\"train_indices = {list(shuffle[2][0])}\")\n",
    "        print(f\"test_indices = {list(shuffle[2][1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b46c40",
   "metadata": {},
   "source": [
    "Go to ```config.yaml``` and set \n",
    "\n",
    "- `batch_size: 1`\n",
    "- `default_track_method: box`\n",
    "\n",
    "\n",
    "Go to ```dlc-models/iteration-2/devMay17-trainset95shuffle1/train/pytorch_config.yaml``` and set \n",
    "\n",
    "- `batch_size: 1`\n",
    "- `device: cpu`\n",
    "- `epochs: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6b8ab",
   "metadata": {},
   "source": [
    "## Cool Machine Learning Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8c072",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d1571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(config_path: dict):\n",
    "    print(80 * \"-\")\n",
    "    print(\"Creating image augmentation transforms\")\n",
    "    cfg = read_yaml(config_path)\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.Affine(\n",
    "                scale=(0.75, 1.5),\n",
    "                rotate=(-30, 30),\n",
    "                translate_px=(-40, 40),\n",
    "            ),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.MotionBlur(),\n",
    "            A.PixelDropout(),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ],\n",
    "        keypoint_params=A.KeypointParams(\n",
    "            format='xy',\n",
    "            remove_invisible=False,\n",
    "            label_fields=['class_labels']\n",
    "        )\n",
    "    )\n",
    "    print(80 * \"-\")\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb41a3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85388c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Creating image augmentation transforms\n",
      "--------------------------------------------------------------------------------\n",
      "Training started\n",
      "The data has been loaded!\n",
      "The data has been loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niels/Documents/upamathis/repos/DLCdev/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py:780: RuntimeWarning: All-NaN slice encountered\n",
      "  bboxes[:, :2] = np.nanmin(data[..., :2], axis=1) - slack  # X1, Y1\n",
      "/Users/niels/Documents/upamathis/repos/DLCdev/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py:781: RuntimeWarning: All-NaN slice encountered\n",
      "  bboxes[:, 2:4] = np.nanmax(data[..., :2], axis=1) + slack  # X2, Y2\n",
      "/Users/niels/Documents/upamathis/repos/DLCdev/deeplabcut/pose_estimation_pytorch/data/dataset.py:204: RuntimeWarning: Mean of empty slice.\n",
      "  keypoints[i, -1, :] = keypoints[i, :-1, :][~np.any(keypoints[i, :-1, :] == -1, axis=1)].mean(axis = 0)\n",
      "/Users/niels/miniconda/envs/upa/lib/python3.9/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 done, starting eval on validation data\n",
      "Epoch 1/100, train loss 0.48323, valid loss 0.51788, lr 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niels/Documents/upamathis/repos/DLCdev/deeplabcut/pose_estimation_pytorch/data/dataset.py:199: RuntimeWarning: Mean of empty slice.\n",
      "  keypoints[:, -1, :] = keypoints[:, :-1, :][~np.any(keypoints[:, :-1, :] == -1, axis=2)].reshape(n_annotations, -1, 2).mean(axis = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 2 done, starting eval on validation data\n",
      "Epoch 2/100, train loss 0.42260, valid loss 0.39681, lr 0.01\n",
      "Training for epoch 3 done, starting eval on validation data\n",
      "Epoch 3/100, train loss 0.42383, valid loss 0.36875, lr 0.01\n",
      "Training for epoch 4 done, starting eval on validation data\n",
      "Epoch 4/100, train loss 0.44283, valid loss 0.34574, lr 0.01\n",
      "Training for epoch 5 done, starting eval on validation data\n",
      "Epoch 5/100, train loss 0.39841, valid loss 0.46031, lr 0.01\n",
      "Training for epoch 6 done, starting eval on validation data\n",
      "Epoch 6/100, train loss 0.39542, valid loss 0.35941, lr 0.01\n",
      "Training for epoch 7 done, starting eval on validation data\n",
      "Epoch 7/100, train loss 0.37296, valid loss 0.37205, lr 0.01\n",
      "Training for epoch 8 done, starting eval on validation data\n",
      "Epoch 8/100, train loss 0.40043, valid loss 0.32555, lr 0.01\n",
      "Training for epoch 9 done, starting eval on validation data\n",
      "Epoch 9/100, train loss 0.33222, valid loss 0.27429, lr 0.01\n",
      "Training for epoch 10 done, starting eval on validation data\n",
      "Epoch 10/100, train loss 0.31838, valid loss 0.34152, lr 0.05\n",
      "Training for epoch 11 done, starting eval on validation data\n",
      "Epoch 11/100, train loss 0.32866, valid loss 0.31681, lr 0.05\n",
      "Training for epoch 12 done, starting eval on validation data\n",
      "Epoch 12/100, train loss 0.31153, valid loss 0.23507, lr 0.05\n",
      "Training for epoch 13 done, starting eval on validation data\n",
      "Epoch 13/100, train loss 0.24491, valid loss 0.25352, lr 0.05\n",
      "Training for epoch 14 done, starting eval on validation data\n",
      "Epoch 14/100, train loss 0.25446, valid loss 0.18299, lr 0.05\n",
      "Training for epoch 15 done, starting eval on validation data\n",
      "Epoch 15/100, train loss 0.18679, valid loss 0.08463, lr 0.05\n",
      "Training for epoch 16 done, starting eval on validation data\n",
      "Epoch 16/100, train loss 0.15456, valid loss 0.12598, lr 0.05\n",
      "Training for epoch 17 done, starting eval on validation data\n",
      "Epoch 17/100, train loss 0.18185, valid loss 0.12112, lr 0.05\n",
      "Training for epoch 18 done, starting eval on validation data\n",
      "Epoch 18/100, train loss 0.11745, valid loss 0.12995, lr 0.05\n",
      "Training for epoch 19 done, starting eval on validation data\n",
      "Epoch 19/100, train loss 0.11283, valid loss 0.12812, lr 0.05\n",
      "Training for epoch 20 done, starting eval on validation data\n",
      "Epoch 20/100, train loss 0.13521, valid loss 0.08800, lr 0.05\n",
      "Training for epoch 21 done, starting eval on validation data\n",
      "Epoch 21/100, train loss 0.16851, valid loss 0.10970, lr 0.05\n",
      "Training for epoch 22 done, starting eval on validation data\n",
      "Epoch 22/100, train loss 0.12864, valid loss 0.07102, lr 0.05\n",
      "Training for epoch 23 done, starting eval on validation data\n",
      "Epoch 23/100, train loss 0.12487, valid loss 0.09178, lr 0.05\n",
      "Training for epoch 24 done, starting eval on validation data\n",
      "Epoch 24/100, train loss 0.11686, valid loss 0.16989, lr 0.05\n",
      "Training for epoch 25 done, starting eval on validation data\n",
      "Epoch 25/100, train loss 0.09719, valid loss 0.08400, lr 0.05\n",
      "Training for epoch 26 done, starting eval on validation data\n",
      "Epoch 26/100, train loss 0.12203, valid loss 0.11128, lr 0.05\n",
      "Training for epoch 27 done, starting eval on validation data\n",
      "Epoch 27/100, train loss 0.12037, valid loss 0.09935, lr 0.05\n",
      "Training for epoch 28 done, starting eval on validation data\n",
      "Epoch 28/100, train loss 0.08876, valid loss 0.14408, lr 0.05\n",
      "Training for epoch 29 done, starting eval on validation data\n",
      "Epoch 29/100, train loss 0.08964, valid loss 0.05177, lr 0.05\n",
      "Training for epoch 30 done, starting eval on validation data\n",
      "Epoch 30/100, train loss 0.06239, valid loss 0.10386, lr 0.05\n",
      "Training for epoch 31 done, starting eval on validation data\n",
      "Epoch 31/100, train loss 0.07905, valid loss 0.16699, lr 0.05\n",
      "Training for epoch 32 done, starting eval on validation data\n",
      "Epoch 32/100, train loss 0.08964, valid loss 0.07776, lr 0.05\n",
      "Training for epoch 33 done, starting eval on validation data\n",
      "Epoch 33/100, train loss 0.07476, valid loss 0.04732, lr 0.05\n",
      "Training for epoch 34 done, starting eval on validation data\n",
      "Epoch 34/100, train loss 0.09393, valid loss 0.08551, lr 0.05\n",
      "Training for epoch 35 done, starting eval on validation data\n",
      "Epoch 35/100, train loss 0.05894, valid loss 0.09183, lr 0.05\n",
      "Training for epoch 36 done, starting eval on validation data\n",
      "Epoch 36/100, train loss 0.05271, valid loss 0.06598, lr 0.05\n",
      "Training for epoch 37 done, starting eval on validation data\n",
      "Epoch 37/100, train loss 0.10012, valid loss 0.05355, lr 0.05\n",
      "Training for epoch 38 done, starting eval on validation data\n",
      "Epoch 38/100, train loss 0.09992, valid loss 0.06720, lr 0.05\n",
      "Training for epoch 39 done, starting eval on validation data\n",
      "Epoch 39/100, train loss 0.09821, valid loss 0.14275, lr 0.05\n",
      "Training for epoch 40 done, starting eval on validation data\n",
      "Epoch 40/100, train loss 0.07467, valid loss 0.19097, lr 0.05\n",
      "Training for epoch 41 done, starting eval on validation data\n",
      "Epoch 41/100, train loss 0.04904, valid loss 0.14484, lr 0.05\n",
      "Training for epoch 42 done, starting eval on validation data\n",
      "Epoch 42/100, train loss 0.08840, valid loss 0.15833, lr 0.05\n",
      "Training for epoch 43 done, starting eval on validation data\n",
      "Epoch 43/100, train loss 0.08567, valid loss 0.17768, lr 0.05\n",
      "Training for epoch 44 done, starting eval on validation data\n",
      "Epoch 44/100, train loss 0.04960, valid loss 0.09033, lr 0.05\n",
      "Training for epoch 45 done, starting eval on validation data\n",
      "Epoch 45/100, train loss 0.06685, valid loss 0.09522, lr 0.05\n",
      "Training for epoch 46 done, starting eval on validation data\n",
      "Epoch 46/100, train loss 0.05998, valid loss 0.05496, lr 0.05\n",
      "Training for epoch 47 done, starting eval on validation data\n",
      "Epoch 47/100, train loss 0.07903, valid loss 0.18571, lr 0.05\n",
      "Training for epoch 48 done, starting eval on validation data\n",
      "Epoch 48/100, train loss 0.09022, valid loss 0.17420, lr 0.05\n",
      "Training for epoch 49 done, starting eval on validation data\n",
      "Epoch 49/100, train loss 0.08071, valid loss 0.09530, lr 0.05\n",
      "Training for epoch 50 done, starting eval on validation data\n",
      "Finished epoch 50; saving model\n",
      "Epoch 50/100, train loss 0.07858, valid loss 0.04333, lr 0.05\n",
      "Training for epoch 51 done, starting eval on validation data\n",
      "Epoch 51/100, train loss 0.05448, valid loss 0.05203, lr 0.05\n",
      "Training for epoch 52 done, starting eval on validation data\n",
      "Epoch 52/100, train loss 0.03771, valid loss 0.04035, lr 0.05\n",
      "Training for epoch 53 done, starting eval on validation data\n",
      "Epoch 53/100, train loss 0.05448, valid loss 0.08736, lr 0.05\n",
      "Training for epoch 54 done, starting eval on validation data\n",
      "Epoch 54/100, train loss 0.05774, valid loss 0.07190, lr 0.05\n",
      "Training for epoch 55 done, starting eval on validation data\n",
      "Epoch 55/100, train loss 0.06573, valid loss 0.09128, lr 0.05\n",
      "Training for epoch 56 done, starting eval on validation data\n",
      "Epoch 56/100, train loss 0.05901, valid loss 0.07137, lr 0.05\n",
      "Training for epoch 57 done, starting eval on validation data\n",
      "Epoch 57/100, train loss 0.04684, valid loss 0.14476, lr 0.05\n",
      "Training for epoch 58 done, starting eval on validation data\n",
      "Epoch 58/100, train loss 0.06537, valid loss 0.05874, lr 0.05\n",
      "Training for epoch 59 done, starting eval on validation data\n",
      "Epoch 59/100, train loss 0.04962, valid loss 0.16930, lr 0.05\n",
      "Training for epoch 60 done, starting eval on validation data\n",
      "Epoch 60/100, train loss 0.02981, valid loss 0.10723, lr 0.05\n",
      "Training for epoch 61 done, starting eval on validation data\n",
      "Epoch 61/100, train loss 0.06254, valid loss 0.10420, lr 0.05\n",
      "Training for epoch 62 done, starting eval on validation data\n",
      "Epoch 62/100, train loss 0.04832, valid loss 0.06080, lr 0.05\n",
      "Training for epoch 63 done, starting eval on validation data\n",
      "Epoch 63/100, train loss 0.06350, valid loss 0.07582, lr 0.05\n",
      "Training for epoch 64 done, starting eval on validation data\n",
      "Epoch 64/100, train loss 0.03820, valid loss 0.13608, lr 0.05\n",
      "Training for epoch 65 done, starting eval on validation data\n",
      "Epoch 65/100, train loss 0.05748, valid loss 0.07652, lr 0.05\n",
      "Training for epoch 66 done, starting eval on validation data\n",
      "Epoch 66/100, train loss 0.07171, valid loss 0.05458, lr 0.05\n",
      "Training for epoch 67 done, starting eval on validation data\n",
      "Epoch 67/100, train loss 0.03908, valid loss 0.05008, lr 0.05\n",
      "Training for epoch 68 done, starting eval on validation data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, train loss 0.04391, valid loss 0.09390, lr 0.05\n",
      "Training for epoch 69 done, starting eval on validation data\n",
      "Epoch 69/100, train loss 0.05402, valid loss 0.08330, lr 0.05\n",
      "Training for epoch 70 done, starting eval on validation data\n",
      "Epoch 70/100, train loss 0.05966, valid loss 0.07258, lr 0.05\n",
      "Training for epoch 71 done, starting eval on validation data\n",
      "Epoch 71/100, train loss 0.04031, valid loss 0.25166, lr 0.05\n",
      "Training for epoch 72 done, starting eval on validation data\n",
      "Epoch 72/100, train loss 0.04912, valid loss 0.06471, lr 0.05\n",
      "Training for epoch 73 done, starting eval on validation data\n",
      "Epoch 73/100, train loss 0.06245, valid loss 0.06946, lr 0.05\n",
      "Training for epoch 74 done, starting eval on validation data\n",
      "Epoch 74/100, train loss 0.04250, valid loss 0.09356, lr 0.05\n",
      "Training for epoch 75 done, starting eval on validation data\n",
      "Epoch 75/100, train loss 0.04344, valid loss 0.06042, lr 0.05\n",
      "Training for epoch 76 done, starting eval on validation data\n",
      "Epoch 76/100, train loss 0.04482, valid loss 0.08781, lr 0.05\n",
      "Training for epoch 77 done, starting eval on validation data\n",
      "Epoch 77/100, train loss 0.02696, valid loss 0.06153, lr 0.05\n",
      "Training for epoch 78 done, starting eval on validation data\n",
      "Epoch 78/100, train loss 0.03860, valid loss 0.14721, lr 0.05\n",
      "Training for epoch 79 done, starting eval on validation data\n",
      "Epoch 79/100, train loss 0.04755, valid loss 0.05753, lr 0.05\n",
      "Training for epoch 80 done, starting eval on validation data\n",
      "Epoch 80/100, train loss 0.04112, valid loss 0.08069, lr 0.05\n",
      "Training for epoch 81 done, starting eval on validation data\n",
      "Epoch 81/100, train loss 0.04298, valid loss 0.20434, lr 0.05\n",
      "Training for epoch 82 done, starting eval on validation data\n",
      "Epoch 82/100, train loss 0.05831, valid loss 0.06052, lr 0.05\n",
      "Training for epoch 83 done, starting eval on validation data\n",
      "Epoch 83/100, train loss 0.06314, valid loss 0.05989, lr 0.05\n",
      "Training for epoch 84 done, starting eval on validation data\n",
      "Epoch 84/100, train loss 0.03660, valid loss 0.07047, lr 0.05\n",
      "Training for epoch 85 done, starting eval on validation data\n",
      "Epoch 85/100, train loss 0.04097, valid loss 0.05826, lr 0.05\n",
      "Training for epoch 86 done, starting eval on validation data\n",
      "Epoch 86/100, train loss 0.04906, valid loss 0.09997, lr 0.05\n",
      "Training for epoch 87 done, starting eval on validation data\n",
      "Epoch 87/100, train loss 0.02902, valid loss 0.14049, lr 0.05\n",
      "Training for epoch 88 done, starting eval on validation data\n",
      "Epoch 88/100, train loss 0.05416, valid loss 0.07298, lr 0.05\n",
      "Training for epoch 89 done, starting eval on validation data\n",
      "Epoch 89/100, train loss 0.05371, valid loss 0.05459, lr 0.05\n",
      "Training for epoch 90 done, starting eval on validation data\n",
      "Epoch 90/100, train loss 0.04274, valid loss 0.11589, lr 0.05\n",
      "Training for epoch 91 done, starting eval on validation data\n",
      "Epoch 91/100, train loss 0.06517, valid loss 0.19256, lr 0.05\n",
      "Training for epoch 92 done, starting eval on validation data\n",
      "Epoch 92/100, train loss 0.04052, valid loss 0.09962, lr 0.05\n",
      "Training for epoch 93 done, starting eval on validation data\n",
      "Epoch 93/100, train loss 0.07607, valid loss 0.06569, lr 0.05\n",
      "Training for epoch 94 done, starting eval on validation data\n",
      "Epoch 94/100, train loss 0.05363, valid loss 0.07790, lr 0.05\n",
      "Training for epoch 95 done, starting eval on validation data\n",
      "Epoch 95/100, train loss 0.08711, valid loss 0.06059, lr 0.05\n",
      "Training for epoch 96 done, starting eval on validation data\n",
      "Epoch 96/100, train loss 0.03173, valid loss 0.08042, lr 0.05\n",
      "Training for epoch 97 done, starting eval on validation data\n",
      "Epoch 97/100, train loss 0.04018, valid loss 0.09979, lr 0.05\n",
      "Training for epoch 98 done, starting eval on validation data\n",
      "Epoch 98/100, train loss 0.03320, valid loss 0.09921, lr 0.05\n",
      "Training for epoch 99 done, starting eval on validation data\n",
      "Epoch 99/100, train loss 0.03489, valid loss 0.07187, lr 0.05\n",
      "Training for epoch 100 done, starting eval on validation data\n",
      "Finished epoch 100; saving model\n",
      "Epoch 100/100, train loss 0.03392, valid loss 0.05536, lr 0.05\n",
      "Finished epoch 100; saving model\n",
      "Training ended after 2860.13 seconds\n"
     ]
    }
   ],
   "source": [
    "def train(config_path, shuffle=1, transform=None, model_prefix=\"\"):\n",
    "    \"\"\"Trains the model and evaluates it on a given dataset\"\"\"\n",
    "    # Training the network\n",
    "    print(\"Training started\")\n",
    "    start_time = time.time()\n",
    "    train_network(\n",
    "        config_path,\n",
    "        shuffle=shuffle,\n",
    "        transform=transform,\n",
    "        model_prefix=model_prefix,\n",
    "    )\n",
    "    delta_time = time.time() - start_time\n",
    "    print(f\"Training ended after {delta_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# No flip transform here; it's confusing the model, better to keep left right for the pen ends\n",
    "transform = get_transform(config_path)\n",
    "train(config_path, shuffle=1, transform=transform, model_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55643247",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d9831c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 5164.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 4608.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 19.43954326501073, 'rmse_pcutoff': 19.43954326501073, 'mAP': 0.16822882288228821, 'mAR': 0.2777777777777778, 'mAP_pcutoff': 0.16822882288228821, 'mAR_pcutoff': 0.2777777777777778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_transform = A.Compose(\n",
    "    [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])],\n",
    "    keypoint_params=A.KeypointParams(\n",
    "        format='xy',\n",
    "        remove_invisible=False,\n",
    "        # label_fields=['class_labels']\n",
    "    )\n",
    ")\n",
    "\n",
    "inference_network(\n",
    "    config_path,\n",
    "    shuffle=1,\n",
    "    model_prefix=model_prefix,\n",
    "    load_epoch=-1,\n",
    "    transform=test_transform,\n",
    "    plot=False,\n",
    "    evaluate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca3335",
   "metadata": {},
   "source": [
    "### Video Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9472257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"{project}/videos/multipen.mov\"\n",
    "output_folder = f\"{project}/videos-analysis-output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1353b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the output folder /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos-analysis-output\n",
      "The data has been loaded!\n",
      "Loading /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos/multipen.mov\n",
      "Video metadata: \n",
      "  n_frames:   167\n",
      "  fps:        29.98\n",
      "  resolution: w=480, h=640\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167/167 [01:34<00:00,  1.77it/s]\n",
      "Inference is done for /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos/multipen.mov! Saving results...\n",
      "Extracting  2 instances per bodypart\n"
     ]
    }
   ],
   "source": [
    "analysis_transform = A.Compose(\n",
    "    [A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])],\n",
    ")\n",
    "\n",
    "results = analyze_videos(\n",
    "    config_path,\n",
    "    data_path=data_path,\n",
    "    output_folder=output_folder,\n",
    "    dataset_index=0,\n",
    "    shuffle=1,\n",
    "    snapshot_index=-1,\n",
    "    model_prefix=model_prefix,\n",
    "    batch_size=1,\n",
    "    device=\"cpu\",\n",
    "    transform=analysis_transform,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72cc60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for multipen.mov\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>scorer</th>\n",
       "      <th colspan=\"12\" halign=\"left\">DLC_dekr_w18_dev-maJun14shuffle1_100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodyparts</th>\n",
       "      <th colspan=\"6\" halign=\"left\">capTip</th>\n",
       "      <th colspan=\"6\" halign=\"left\">bottom</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coords</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>likelihood2</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>likelihood</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>likelihood2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>412.565735</td>\n",
       "      <td>323.817657</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>136.743439</td>\n",
       "      <td>367.157104</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>350.835297</td>\n",
       "      <td>221.351257</td>\n",
       "      <td>0.888584</td>\n",
       "      <td>104.484306</td>\n",
       "      <td>248.616211</td>\n",
       "      <td>0.888584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>412.958649</td>\n",
       "      <td>322.527863</td>\n",
       "      <td>0.910575</td>\n",
       "      <td>137.775055</td>\n",
       "      <td>366.411591</td>\n",
       "      <td>0.910575</td>\n",
       "      <td>351.106781</td>\n",
       "      <td>220.503830</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>105.661949</td>\n",
       "      <td>248.001266</td>\n",
       "      <td>0.890442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>411.865601</td>\n",
       "      <td>321.174103</td>\n",
       "      <td>0.908754</td>\n",
       "      <td>138.779602</td>\n",
       "      <td>365.509796</td>\n",
       "      <td>0.908754</td>\n",
       "      <td>352.933746</td>\n",
       "      <td>220.065018</td>\n",
       "      <td>0.905753</td>\n",
       "      <td>103.932434</td>\n",
       "      <td>247.258759</td>\n",
       "      <td>0.905753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354.574585</td>\n",
       "      <td>219.122620</td>\n",
       "      <td>0.909724</td>\n",
       "      <td>101.394585</td>\n",
       "      <td>245.989212</td>\n",
       "      <td>0.909724</td>\n",
       "      <td>411.770874</td>\n",
       "      <td>320.520874</td>\n",
       "      <td>0.906561</td>\n",
       "      <td>138.972931</td>\n",
       "      <td>363.692780</td>\n",
       "      <td>0.906561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410.311035</td>\n",
       "      <td>319.990356</td>\n",
       "      <td>0.907651</td>\n",
       "      <td>138.996521</td>\n",
       "      <td>362.606537</td>\n",
       "      <td>0.907651</td>\n",
       "      <td>354.305389</td>\n",
       "      <td>218.376434</td>\n",
       "      <td>0.903840</td>\n",
       "      <td>103.324509</td>\n",
       "      <td>243.363358</td>\n",
       "      <td>0.903840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>410.643921</td>\n",
       "      <td>318.202362</td>\n",
       "      <td>0.911764</td>\n",
       "      <td>137.130692</td>\n",
       "      <td>359.627502</td>\n",
       "      <td>0.911764</td>\n",
       "      <td>355.519592</td>\n",
       "      <td>217.095596</td>\n",
       "      <td>0.904601</td>\n",
       "      <td>99.158440</td>\n",
       "      <td>240.248077</td>\n",
       "      <td>0.904601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>409.742981</td>\n",
       "      <td>316.380249</td>\n",
       "      <td>0.901055</td>\n",
       "      <td>133.235260</td>\n",
       "      <td>356.609375</td>\n",
       "      <td>0.901055</td>\n",
       "      <td>353.225220</td>\n",
       "      <td>215.458969</td>\n",
       "      <td>0.892867</td>\n",
       "      <td>99.384834</td>\n",
       "      <td>237.368530</td>\n",
       "      <td>0.892867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>408.228027</td>\n",
       "      <td>312.733154</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>133.103302</td>\n",
       "      <td>355.506714</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>351.742432</td>\n",
       "      <td>213.533508</td>\n",
       "      <td>0.887378</td>\n",
       "      <td>96.909691</td>\n",
       "      <td>235.491638</td>\n",
       "      <td>0.887378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>405.896362</td>\n",
       "      <td>310.368164</td>\n",
       "      <td>0.905041</td>\n",
       "      <td>131.622696</td>\n",
       "      <td>354.189941</td>\n",
       "      <td>0.905041</td>\n",
       "      <td>350.563843</td>\n",
       "      <td>209.431061</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>97.333336</td>\n",
       "      <td>232.715561</td>\n",
       "      <td>0.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>403.661713</td>\n",
       "      <td>307.417633</td>\n",
       "      <td>0.915674</td>\n",
       "      <td>132.464645</td>\n",
       "      <td>351.839294</td>\n",
       "      <td>0.915674</td>\n",
       "      <td>349.526245</td>\n",
       "      <td>204.167603</td>\n",
       "      <td>0.870041</td>\n",
       "      <td>99.534515</td>\n",
       "      <td>229.237091</td>\n",
       "      <td>0.870041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>402.249878</td>\n",
       "      <td>305.801117</td>\n",
       "      <td>0.902103</td>\n",
       "      <td>131.420425</td>\n",
       "      <td>349.499115</td>\n",
       "      <td>0.902103</td>\n",
       "      <td>350.183472</td>\n",
       "      <td>202.777832</td>\n",
       "      <td>0.883109</td>\n",
       "      <td>96.153610</td>\n",
       "      <td>230.004837</td>\n",
       "      <td>0.883109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>404.913025</td>\n",
       "      <td>303.809113</td>\n",
       "      <td>0.908145</td>\n",
       "      <td>130.677612</td>\n",
       "      <td>346.870453</td>\n",
       "      <td>0.908145</td>\n",
       "      <td>349.494019</td>\n",
       "      <td>200.670456</td>\n",
       "      <td>0.880896</td>\n",
       "      <td>97.404152</td>\n",
       "      <td>227.906815</td>\n",
       "      <td>0.880896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>405.885071</td>\n",
       "      <td>301.742523</td>\n",
       "      <td>0.905462</td>\n",
       "      <td>127.604477</td>\n",
       "      <td>342.251648</td>\n",
       "      <td>0.905462</td>\n",
       "      <td>348.250793</td>\n",
       "      <td>198.702271</td>\n",
       "      <td>0.873236</td>\n",
       "      <td>95.567566</td>\n",
       "      <td>222.637390</td>\n",
       "      <td>0.873236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>404.897644</td>\n",
       "      <td>298.452606</td>\n",
       "      <td>0.892197</td>\n",
       "      <td>128.125961</td>\n",
       "      <td>338.327240</td>\n",
       "      <td>0.892197</td>\n",
       "      <td>343.893799</td>\n",
       "      <td>196.659180</td>\n",
       "      <td>0.875633</td>\n",
       "      <td>94.761703</td>\n",
       "      <td>220.120163</td>\n",
       "      <td>0.875633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>403.209381</td>\n",
       "      <td>295.977905</td>\n",
       "      <td>0.908151</td>\n",
       "      <td>128.247467</td>\n",
       "      <td>335.467590</td>\n",
       "      <td>0.908151</td>\n",
       "      <td>343.696228</td>\n",
       "      <td>193.415207</td>\n",
       "      <td>0.867187</td>\n",
       "      <td>96.964317</td>\n",
       "      <td>219.475052</td>\n",
       "      <td>0.867187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>400.744324</td>\n",
       "      <td>291.131470</td>\n",
       "      <td>0.906575</td>\n",
       "      <td>131.059952</td>\n",
       "      <td>333.776001</td>\n",
       "      <td>0.906575</td>\n",
       "      <td>341.200134</td>\n",
       "      <td>189.326904</td>\n",
       "      <td>0.883586</td>\n",
       "      <td>96.958824</td>\n",
       "      <td>216.403366</td>\n",
       "      <td>0.883586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>344.122131</td>\n",
       "      <td>186.372833</td>\n",
       "      <td>0.903969</td>\n",
       "      <td>93.799080</td>\n",
       "      <td>213.406998</td>\n",
       "      <td>0.903969</td>\n",
       "      <td>402.646423</td>\n",
       "      <td>288.347687</td>\n",
       "      <td>0.895400</td>\n",
       "      <td>131.376678</td>\n",
       "      <td>330.318207</td>\n",
       "      <td>0.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>400.504578</td>\n",
       "      <td>283.709534</td>\n",
       "      <td>0.895093</td>\n",
       "      <td>127.885971</td>\n",
       "      <td>324.421539</td>\n",
       "      <td>0.895093</td>\n",
       "      <td>344.404297</td>\n",
       "      <td>185.375595</td>\n",
       "      <td>0.884364</td>\n",
       "      <td>91.906067</td>\n",
       "      <td>208.068985</td>\n",
       "      <td>0.884364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>397.058044</td>\n",
       "      <td>279.070221</td>\n",
       "      <td>0.896414</td>\n",
       "      <td>127.549118</td>\n",
       "      <td>320.940125</td>\n",
       "      <td>0.896414</td>\n",
       "      <td>341.931030</td>\n",
       "      <td>181.669434</td>\n",
       "      <td>0.875879</td>\n",
       "      <td>88.196793</td>\n",
       "      <td>202.836334</td>\n",
       "      <td>0.875879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>395.021912</td>\n",
       "      <td>273.121216</td>\n",
       "      <td>0.899532</td>\n",
       "      <td>129.742401</td>\n",
       "      <td>317.492157</td>\n",
       "      <td>0.899532</td>\n",
       "      <td>341.478577</td>\n",
       "      <td>172.787689</td>\n",
       "      <td>0.859416</td>\n",
       "      <td>93.101646</td>\n",
       "      <td>196.606903</td>\n",
       "      <td>0.859416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "scorer    DLC_dekr_w18_dev-maJun14shuffle1_100                         \\\n",
       "bodyparts                               capTip                          \n",
       "coords                                       x           y likelihood   \n",
       "0                                   412.565735  323.817657   0.913636   \n",
       "1                                   412.958649  322.527863   0.910575   \n",
       "2                                   411.865601  321.174103   0.908754   \n",
       "3                                   354.574585  219.122620   0.909724   \n",
       "4                                   410.311035  319.990356   0.907651   \n",
       "5                                   410.643921  318.202362   0.911764   \n",
       "6                                   409.742981  316.380249   0.901055   \n",
       "7                                   408.228027  312.733154   0.898148   \n",
       "8                                   405.896362  310.368164   0.905041   \n",
       "9                                   403.661713  307.417633   0.915674   \n",
       "10                                  402.249878  305.801117   0.902103   \n",
       "11                                  404.913025  303.809113   0.908145   \n",
       "12                                  405.885071  301.742523   0.905462   \n",
       "13                                  404.897644  298.452606   0.892197   \n",
       "14                                  403.209381  295.977905   0.908151   \n",
       "15                                  400.744324  291.131470   0.906575   \n",
       "16                                  344.122131  186.372833   0.903969   \n",
       "17                                  400.504578  283.709534   0.895093   \n",
       "18                                  397.058044  279.070221   0.896414   \n",
       "19                                  395.021912  273.121216   0.899532   \n",
       "\n",
       "scorer                                                                 \\\n",
       "bodyparts                                          bottom               \n",
       "coords             x2          y2 likelihood2           x           y   \n",
       "0          136.743439  367.157104    0.913636  350.835297  221.351257   \n",
       "1          137.775055  366.411591    0.910575  351.106781  220.503830   \n",
       "2          138.779602  365.509796    0.908754  352.933746  220.065018   \n",
       "3          101.394585  245.989212    0.909724  411.770874  320.520874   \n",
       "4          138.996521  362.606537    0.907651  354.305389  218.376434   \n",
       "5          137.130692  359.627502    0.911764  355.519592  217.095596   \n",
       "6          133.235260  356.609375    0.901055  353.225220  215.458969   \n",
       "7          133.103302  355.506714    0.898148  351.742432  213.533508   \n",
       "8          131.622696  354.189941    0.905041  350.563843  209.431061   \n",
       "9          132.464645  351.839294    0.915674  349.526245  204.167603   \n",
       "10         131.420425  349.499115    0.902103  350.183472  202.777832   \n",
       "11         130.677612  346.870453    0.908145  349.494019  200.670456   \n",
       "12         127.604477  342.251648    0.905462  348.250793  198.702271   \n",
       "13         128.125961  338.327240    0.892197  343.893799  196.659180   \n",
       "14         128.247467  335.467590    0.908151  343.696228  193.415207   \n",
       "15         131.059952  333.776001    0.906575  341.200134  189.326904   \n",
       "16          93.799080  213.406998    0.903969  402.646423  288.347687   \n",
       "17         127.885971  324.421539    0.895093  344.404297  185.375595   \n",
       "18         127.549118  320.940125    0.896414  341.931030  181.669434   \n",
       "19         129.742401  317.492157    0.899532  341.478577  172.787689   \n",
       "\n",
       "scorer                                                    \n",
       "bodyparts                                                 \n",
       "coords    likelihood          x2          y2 likelihood2  \n",
       "0           0.888584  104.484306  248.616211    0.888584  \n",
       "1           0.890442  105.661949  248.001266    0.890442  \n",
       "2           0.905753  103.932434  247.258759    0.905753  \n",
       "3           0.906561  138.972931  363.692780    0.906561  \n",
       "4           0.903840  103.324509  243.363358    0.903840  \n",
       "5           0.904601   99.158440  240.248077    0.904601  \n",
       "6           0.892867   99.384834  237.368530    0.892867  \n",
       "7           0.887378   96.909691  235.491638    0.887378  \n",
       "8           0.865385   97.333336  232.715561    0.865385  \n",
       "9           0.870041   99.534515  229.237091    0.870041  \n",
       "10          0.883109   96.153610  230.004837    0.883109  \n",
       "11          0.880896   97.404152  227.906815    0.880896  \n",
       "12          0.873236   95.567566  222.637390    0.873236  \n",
       "13          0.875633   94.761703  220.120163    0.875633  \n",
       "14          0.867187   96.964317  219.475052    0.867187  \n",
       "15          0.883586   96.958824  216.403366    0.883586  \n",
       "16          0.895400  131.376678  330.318207    0.895400  \n",
       "17          0.884364   91.906067  208.068985    0.884364  \n",
       "18          0.875879   88.196793  202.836334    0.875879  \n",
       "19          0.859416   93.101646  196.606903    0.859416  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Results for {Path(results[0][0]).name}\")\n",
    "results[0][1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ad84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/dlc-models/iteration-0/dev-maJun14-trainset80shuffle1/train/snapshot-100.pt for model /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/dlc-models/iteration-0/dev-maJun14-trainset80shuffle1\n",
      "Processing...  /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos/multipen.mov\n",
      "Loading From /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos-analysis-output/multipenDLC_dekr_w18_dev-maJun14shuffle1_100.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [00:00, 3599.74it/s]\n",
      "/Users/niels/Documents/upamathis/repos/DLCdev/deeplabcut/refine_training_dataset/stitch.py:138: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return mode(self.data[..., 3], axis=None, nan_policy=\"omit\")[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tracklets were created (i.e., under the hood deeplabcut.convert_detections2tracklets was run). Now you can 'refine_tracklets' in the GUI, or run 'deeplabcut.stitch_tracklets'.\n",
      "Processing...  /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos/multipen.mov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 14246.96it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_detections2tracklets(\n",
    "    config_path,\n",
    "    str(data_path),\n",
    "    dataset_index=0,\n",
    "    shuffle=1,\n",
    "    output_folder=output_folder,\n",
    "    modelprefix=\"\",\n",
    "    track_method=\"box\",\n",
    ")\n",
    "deeplabcut.stitch_tracklets(\n",
    "    str(config_path),\n",
    "    [str(data_path)],\n",
    "    shuffle=1,\n",
    "    trainingsetindex=0,\n",
    "    destfolder=str(output_folder),\n",
    "    n_tracks=2,\n",
    "    modelprefix=\"\",\n",
    "    save_as_csv=True,\n",
    "    track_method=\"box\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459d342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.1...\n",
      "Starting to process video: /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos/multipen.mov\n",
      "Loading /Users/niels/Documents/upamathis/datasets/dev-pytorch-ma-pen-2023-06-14/videos/multipen.mov and data.\n",
      "Duration of video [s]: 5.57, recorded with 29.98 fps!\n",
      "Overall # of frames: 167 with cropped frame dimensions: 480 640\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:00<00:00, 505.88it/s]\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.create_labeled_video(\n",
    "    config_path,\n",
    "    [data_path],\n",
    "    videotype=\".mov\",\n",
    "    shuffle=1,\n",
    "    trainingsetindex=0,\n",
    "    destfolder=output_folder,\n",
    "    modelprefix=\"\",\n",
    "    track_method=\"box\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3eb3d",
   "metadata": {},
   "source": [
    "## Config Files\n",
    "\n",
    "### `config.yaml`\n",
    "\n",
    "```yaml\n",
    "    # Project definitions (do not edit)\n",
    "Task: dev-ma\n",
    "scorer: pen\n",
    "date: Jun14\n",
    "multianimalproject: true\n",
    "identity: false\n",
    "\n",
    "# Project path (change when moving around)\n",
    "project_path: .../datasets/dev-pytorch-ma-pen-2023-06-14\n",
    "\n",
    "# Annotation data set configuration (and individual video cropping parameters)\n",
    "video_sets:\n",
    "  .../dev-ma-pen-2023-06-14/videos/multipen.mov:\n",
    "    crop: 0, 480, 0, 640\n",
    "individuals:\n",
    "- individual1\n",
    "- individual2\n",
    "uniquebodyparts: []\n",
    "multianimalbodyparts:\n",
    "- capTip\n",
    "- bottom\n",
    "bodyparts: MULTI!\n",
    "\n",
    "# Fraction of video to start/stop when extracting frames for labeling/refinement\n",
    "start: 0\n",
    "stop: 1\n",
    "numframes2pick: 25\n",
    "\n",
    "# Plotting configuration\n",
    "skeleton: []\n",
    "skeleton_color: black\n",
    "pcutoff: 0.6\n",
    "dotsize: 12\n",
    "alphavalue: 0.7\n",
    "colormap: rainbow\n",
    "\n",
    "# Training,Evaluation and Analysis configuration\n",
    "TrainingFraction:\n",
    "- 0.8\n",
    "iteration: 0\n",
    "default_net_type: dekr_w18\n",
    "default_augmenter: multi-animal-imgaug\n",
    "default_track_method: box\n",
    "snapshotindex: -1\n",
    "batch_size: 1\n",
    "\n",
    "# Cropping Parameters (for analysis and outlier frame detection)\n",
    "cropping: false\n",
    "# if cropping is true for analysis, then set the values here:\n",
    "x1: 0\n",
    "x2: 640\n",
    "y1: 277\n",
    "y2: 624\n",
    "\n",
    "# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)\n",
    "corner2move2:\n",
    "- 50\n",
    "- 50\n",
    "move2corner: true\n",
    "```\n",
    "\n",
    "### `train/pytorch_config.yaml`\n",
    "\n",
    "```yaml\n",
    "batch_size: 1\n",
    "cfg_path: \n",
    "  .../datasets/dev-pytorch-ma-pen-2023-06-14/config.yaml\n",
    "criterion:\n",
    "  locref_huber_loss: true\n",
    "  loss_weight_locref: 0.02\n",
    "  type: PoseLoss\n",
    "data:\n",
    "  covering: true\n",
    "  gaussian_noise: 12.75\n",
    "  hist_eq: true\n",
    "  motion_blur: true\n",
    "  normalize_images: true\n",
    "  rotation: 30\n",
    "  scale_jitter:\n",
    "  - 0.5\n",
    "  - 1.25\n",
    "  translation: 40\n",
    "device: cpu\n",
    "display_iters: 1000\n",
    "epochs: 100\n",
    "model:\n",
    "  backbone:\n",
    "    type: HRNet\n",
    "    model_name: hrnet_w18\n",
    "  heatmap_head:\n",
    "    type: HeatmapDEKRHead\n",
    "    channels:\n",
    "    - 270\n",
    "    - 64\n",
    "    - 3\n",
    "    num_blocks: 1\n",
    "    dilation_rate: 1\n",
    "    final_conv_kernel: 1\n",
    "  locref_head:\n",
    "    type: OffsetDEKRHead\n",
    "    channels:\n",
    "    - 270\n",
    "    - 30\n",
    "    - 2\n",
    "    num_offset_per_kpt: 15\n",
    "    num_blocks: 1\n",
    "    dilation_rate: 1\n",
    "    final_conv_kernel: 1\n",
    "  pose_model:\n",
    "    stride: 8\n",
    "  target_generator:\n",
    "    type: DEKRGenerator\n",
    "    num_joints: 2\n",
    "    pos_dist_thresh: 17\n",
    "optimizer:\n",
    "  params:\n",
    "    lr: 0.01\n",
    "  type: SGD\n",
    "pos_dist_thresh: 17\n",
    "predictor:\n",
    "  type: DEKRPredictor\n",
    "  num_animals: 2\n",
    "save_epochs: 50\n",
    "scheduler:\n",
    "  params:\n",
    "    lr_list:\n",
    "    - - 0.05\n",
    "    - - 0.005\n",
    "    milestones:\n",
    "    - 10\n",
    "    - 430\n",
    "  type: LRListScheduler\n",
    "seed: 42\n",
    "solver:\n",
    "  type: BottomUpSingleAnimalSolver\n",
    "with_center: true\n",
    "project_path: .../datasets/dev-pytorch-ma-pen-2023-06-14\n",
    "pose_cfg_path: \n",
    "  .../datasets/dev-pytorch-ma-pen-2023-06-14/dlc-models/iteration-0/dev-maJun14-trainset80shuffle1/train/pose_cfg.yaml\n",
    "```\n",
    "\n",
    "### `train/pose_cfg.yaml`\n",
    "\n",
    "```yaml\n",
    "all_joints:\n",
    "- - 0\n",
    "- - 1\n",
    "all_joints_names:\n",
    "- capTip\n",
    "- bottom\n",
    "alpha_r: 0.02\n",
    "apply_prob: 0.5\n",
    "batch_size: 8\n",
    "contrast:\n",
    "  clahe: true\n",
    "  claheratio: 0.1\n",
    "  histeq: true\n",
    "  histeqratio: 0.1\n",
    "convolution:\n",
    "  edge: false\n",
    "  emboss:\n",
    "    alpha:\n",
    "    - 0.0\n",
    "    - 1.0\n",
    "    strength:\n",
    "    - 0.5\n",
    "    - 1.5\n",
    "  embossratio: 0.1\n",
    "  sharpen: false\n",
    "  sharpenratio: 0.3\n",
    "crop_sampling: hybrid\n",
    "crop_size:\n",
    "- 400\n",
    "- 400\n",
    "cropratio: 0.4\n",
    "dataset: training-datasets/iteration-0/UnaugmentedDataSet_dev-maJun14/dev-ma_pen80shuffle1.pickle\n",
    "dataset_type: multi-animal-imgaug\n",
    "decay_steps: 30000\n",
    "display_iters: 500\n",
    "global_scale: 0.8\n",
    "init_weights: .../DLCdev/deeplabcut\n",
    "intermediate_supervision: false\n",
    "intermediate_supervision_layer: 12\n",
    "location_refinement: true\n",
    "locref_huber_loss: true\n",
    "locref_loss_weight: 0.05\n",
    "locref_stdev: 7.2801\n",
    "lr_init: 0.0005\n",
    "max_input_size: 1500\n",
    "max_shift: 0.4\n",
    "metadataset: training-datasets/iteration-0/UnaugmentedDataSet_dev-maJun14/Documentation_data-dev-ma_80shuffle1.pickle\n",
    "min_input_size: 64\n",
    "mirror: false\n",
    "multi_stage: false\n",
    "multi_step:\n",
    "- - 0.0001\n",
    "  - 7500\n",
    "- - 5.0e-05\n",
    "  - 12000\n",
    "- - 1.0e-05\n",
    "  - 200000\n",
    "net_type: dekr_w18\n",
    "num_idchannel: 0\n",
    "num_joints: 2\n",
    "num_limbs: 1\n",
    "optimizer: adam\n",
    "pafwidth: 20\n",
    "pairwise_huber_loss: false\n",
    "pairwise_loss_weight: 0.1\n",
    "pairwise_predict: false\n",
    "partaffinityfield_graph:\n",
    "- - 0\n",
    "  - 1\n",
    "partaffinityfield_predict: true\n",
    "pos_dist_thresh: 17\n",
    "pre_resize: []\n",
    "project_path: .../datasets/dev-pytorch-ma-pen-2023-06-14\n",
    "rotation: 25\n",
    "rotratio: 0.4\n",
    "save_iters: 10000\n",
    "scale_jitter_lo: 0.5\n",
    "scale_jitter_up: 1.25\n",
    "weigh_only_present_joints: false\n",
    "```\n",
    "\n",
    "### `test/inference_cfg.yaml`\n",
    "\n",
    "```yaml\n",
    "boundingboxslack: 0\n",
    "iou_threshold: 0.6\n",
    "max_age: 1\n",
    "method: m1\n",
    "min_hits: 1\n",
    "minimalnumberofconnections: 1\n",
    "pafthreshold: 0.1\n",
    "pcutoff: 0.1\n",
    "topktoretain: 2\n",
    "variant: 0\n",
    "withid: false\n",
    "```\n",
    "\n",
    "### `test/pose_cfg.yaml`\n",
    "\n",
    "```yaml\n",
    "all_joints:\n",
    "- - 0\n",
    "- - 1\n",
    "all_joints_names:\n",
    "- capTip\n",
    "- bottom\n",
    "dataset: training-datasets/iteration-0/UnaugmentedDataSet_dev-maJun14/dev-ma_pen80shuffle1.pickle\n",
    "dataset_type: multi-animal-imgaug\n",
    "global_scale: 0.8\n",
    "init_weights: .../DLCdev/deeplabcut\n",
    "location_refinement: true\n",
    "locref_smooth: false\n",
    "locref_stdev: 7.2801\n",
    "minconfidence: 0.01\n",
    "multi_stage: false\n",
    "net_type: dekr_w18\n",
    "nmsradius: 5.0\n",
    "num_idchannel: 0\n",
    "num_joints: 2\n",
    "num_limbs: 1\n",
    "pairwise_predict: false\n",
    "partaffinityfield_graph:\n",
    "- - 0\n",
    "  - 1\n",
    "partaffinityfield_predict: true\n",
    "scoremap_dir: test\n",
    "sigma: 1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
