{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a79e50",
   "metadata": {},
   "source": [
    "# Unit Testing in DLC Pytorch Dev using Pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ef0f9",
   "metadata": {},
   "source": [
    "Unit Testing tutorial by Arjan Codes: https://www.youtube.com/watch?v=ULxMQ57engo&t=460s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d3290",
   "metadata": {},
   "source": [
    "Test scripts at `DLCDev/tests/`\n",
    "\n",
    "Base tests on Shaokai's test file named `test_dataset_augmentation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shaokai's test_dataset_augmentation.py\n",
    "# DeepLabCut Toolbox (deeplabcut.org)\n",
    "# © A. & M.W. Mathis Labs\n",
    "# https://github.com/DeepLabCut/DeepLabCut\n",
    "#\n",
    "# Please see AUTHORS for contributors.\n",
    "# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS\n",
    "#\n",
    "# Licensed under GNU Lesser General Public License v3.0\n",
    "#\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import pytest\n",
    "from deeplabcut.pose_estimation_tensorflow.datasets import augmentation\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"width, height\",\n",
    "    [\n",
    "        (200, 200),\n",
    "        (300, 300),\n",
    "        (400, 400),\n",
    "    ],\n",
    ")\n",
    "def test_keypoint_aware_cropping(\n",
    "    sample_image,\n",
    "    sample_keypoints,\n",
    "    width,\n",
    "    height,\n",
    "):\n",
    "    aug = augmentation.KeypointAwareCropToFixedSize(width=width, height=height)\n",
    "    images_aug, keypoints_aug = aug(\n",
    "        images=[sample_image],\n",
    "        keypoints=[sample_keypoints],\n",
    "    )\n",
    "    assert len(images_aug) == len(keypoints_aug) == 1\n",
    "    assert all(im.shape[:2] == (height, width) for im in images_aug)\n",
    "    # Ensure at least a keypoint is visible in each crop\n",
    "    assert all(len(kpts) for kpts in keypoints_aug)\n",
    "\n",
    "    # Test passing in a batch of frames\n",
    "    n_samples = 8\n",
    "    images_aug, keypoints_aug = aug(\n",
    "        images=[sample_image] * n_samples,\n",
    "        keypoints=[sample_keypoints] * n_samples,\n",
    "    )\n",
    "    assert len(images_aug) == len(keypoints_aug) == n_samples\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"width, height\",\n",
    "    [\n",
    "        (200, 200),\n",
    "        (300, 300),\n",
    "        (400, 400),\n",
    "    ],\n",
    ")\n",
    "def test_sequential(\n",
    "    sample_image,\n",
    "    sample_keypoints,\n",
    "    width,\n",
    "    height,\n",
    "):\n",
    "    # Guarantee that images smaller than crop size are handled fine\n",
    "    very_small_image = sample_image[:50, :50]\n",
    "    aug = iaa.Sequential(\n",
    "        [\n",
    "            iaa.PadToFixedSize(width, height),\n",
    "            augmentation.KeypointAwareCropToFixedSize(width, height),\n",
    "        ]\n",
    "    )\n",
    "    images_aug, keypoints_aug = aug(\n",
    "        images=[very_small_image],\n",
    "        keypoints=[sample_keypoints],\n",
    "    )\n",
    "    assert len(images_aug) == len(keypoints_aug) == 1\n",
    "    assert all(im.shape[:2] == (height, width) for im in images_aug)\n",
    "    # Ensure at least a keypoint is visible in each crop\n",
    "    assert all(len(kpts) for kpts in keypoints_aug)\n",
    "\n",
    "    # Test passing in a batch of frames\n",
    "    n_samples = 8\n",
    "    images_aug, keypoints_aug = aug(\n",
    "        images=[very_small_image] * n_samples,\n",
    "        keypoints=[sample_keypoints] * n_samples,\n",
    "    )\n",
    "    assert len(images_aug) == len(keypoints_aug) == n_samples\n",
    "\n",
    "\n",
    "def test_keypoint_horizontal_flip(\n",
    "    sample_image,\n",
    "    sample_keypoints,\n",
    "):\n",
    "    keypoints_flipped = sample_keypoints.copy()\n",
    "    keypoints_flipped[:, 0] = sample_image.shape[1] - keypoints_flipped[:, 0]\n",
    "    pairs = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11)]\n",
    "    aug = augmentation.KeypointFliplr(\n",
    "        keypoints=list(map(str, range(12))),\n",
    "        symmetric_pairs=pairs,\n",
    "    )\n",
    "    keypoints_aug = aug(images=[sample_image], keypoints=[sample_keypoints],)[\n",
    "        1\n",
    "    ][0]\n",
    "    temp = keypoints_aug.reshape((3, 12, 2))\n",
    "    for pair in pairs:\n",
    "        temp[:, pair] = temp[:, pair[::-1]]\n",
    "    keypoints_unaug = temp.reshape((-1, 2))\n",
    "    np.testing.assert_allclose(keypoints_unaug, keypoints_flipped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c841af",
   "metadata": {},
   "source": [
    "Arjan's tips:\n",
    "1. luhn_checksum - test for credit card numbers and dates (ref: https://www.geeksforgeeks.org/luhn-algorithm/)\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d61085",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3-m pytest\n",
    "(base) raeさんは ~に >> python3 -m pytest                         と言います!★ \n",
    "============================= test session starts ==============================\n",
    "platform darwin -- Python 3.10.9, pytest-7.1.2, pluggy-1.0.0\n",
    "rootdir: /Users/rae\n",
    "plugins: anyio-3.5.0\n",
    "collected 0 items / 1 error                                                    \n",
    "\n",
    "==================================== ERRORS ====================================\n",
    "________________________ ERROR collecting test session _________________________\n",
    "/usr/local/anaconda3/lib/python3.10/importlib/__init__.py:126: in import_module\n",
    "    return _bootstrap._gcd_import(name[level:], package, level)\n",
    "<frozen importlib._bootstrap>:1050: in _gcd_import\n",
    "    ???\n",
    "<frozen importlib._bootstrap>:1027: in _find_and_load\n",
    "    ???\n",
    "<frozen importlib._bootstrap>:1006: in _find_and_load_unlocked\n",
    "    ???\n",
    "<frozen importlib._bootstrap>:688: in _load_unlocked\n",
    "    ???\n",
    "/usr/local/anaconda3/lib/python3.10/site-packages/_pytest/assertion/rewrite.py:168: in exec_module\n",
    "    exec(co, module.__dict__)\n",
    "Desktop/DLCdev/tests/conftest.py:18: in <module>\n",
    "    from deeplabcut.pose_estimation_tensorflow.lib import inferenceutils\n",
    "E   ModuleNotFoundError: No module named 'deeplabcut'\n",
    "=========================== short test summary info ============================\n",
    "ERROR  - ModuleNotFoundError: No module named 'deeplabcut'\n",
    "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
    "=============================== 1 error in 0.48s ===============================\n",
    "(base) raeさんは ~に >> cd Desktop                                と言います!★ \n",
    "(base) raeさんは Desktopに >> cd DLCdev                           と言います!★ \n",
    "(base) raeさんは DLCdevに >> cd tests                             と言います!★ \n",
    "(base) raeさんは testsに >> python3 -m pytest                     と言います!★ \n",
    "ImportError while loading conftest '/Users/rae/Desktop/DLCdev/tests/conftest.py'.\n",
    "conftest.py:18: in <module>\n",
    "    from deeplabcut.pose_estimation_tensorflow.lib import inferenceutils\n",
    "E   ModuleNotFoundError: No module named 'deeplabcut'\n",
    "(base) raeさんは testsに >> conda activate dlc                    と言います!★ \n",
    "(dlc) raeさんは testsに >> python3 -m pytest                      と言います!★ \n",
    "Extracting: 100%|█████████████████████████████| 43/43 [00:00<00:00, 1074.71it/s]\n",
    "============================= test session starts ==============================\n",
    "platform darwin -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0\n",
    "rootdir: /Users/rae/Desktop/DLCdev\n",
    "plugins: npe2-0.7.0, napari-0.4.17, napari-plugin-engine-0.2.0\n",
    "collected 115 items                                                            \n",
    "\n",
    "test_auxfun_models.py ..                                                 [  1%]\n",
    "test_auxfun_multianimal.py .F                                            [  3%]\n",
    "test_auxiliaryfunctions.py ..                                            [  5%]\n",
    "test_conversioncode.py .                                                 [  6%]\n",
    "test_crossvalutils.py ...F                                               [  9%]\n",
    "test_dataset_augmentation.py .......                                     [ 15%]\n",
    "test_inferenceutils.py ............                                      [ 26%]\n",
    "test_pose_multianimal_imgaug.py ...............                          [ 39%]\n",
    "test_predict_multianimal.py ...                                          [ 41%]\n",
    "test_stitcher.py ........................F.                              [ 64%]\n",
    "test_trackingutils.py ..........                                                              [ 73%]\n",
    "test_trainingsetmanipulation.py ....                                                          [ 76%]\n",
    "test_triangulation.py .FFFFFF                                                                                  [ 82%]\n",
    "test_video.py .F.F...........FFFFF                                                                              [100%]\n",
    "\n",
    "====================================================== FAILURES =======================================================\n",
    "___________________________________________ test_reorder_individuals_in_df ____________________________________________\n",
    "\n",
    "    def test_reorder_individuals_in_df():\n",
    "        import random\n",
    "    \n",
    "        # Load sample multi animal data\n",
    ">       df = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_auxfun_multianimal.py:43: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "_________________________________________ test_benchmark_paf_graphs_montblanc _________________________________________\n",
    "\n",
    "evaluation_data_and_metadata_montblanc = ({'labeled-data/montblanc/img0000.png': {'groundtruth': [[array(['bird1'], dtype=object), array(['bird1'], dtype=objec...}, 'Scorer': 'DLC_resnet50_MontBlancDec16shuffle1_30000', 'testIndices': array([13, 25]), 'trainFraction': 0.95, ...}})\n",
    "\n",
    "    def test_benchmark_paf_graphs_montblanc(evaluation_data_and_metadata_montblanc):\n",
    "        data, metadata = evaluation_data_and_metadata_montblanc\n",
    "        cfg = {\n",
    "            \"individuals\": [f\"bird{i}\" for i in range(1, 9)],\n",
    "            \"uniquebodyparts\": [\"center\"],\n",
    "            \"multianimalbodyparts\": [\n",
    "                \"head\",\n",
    "                \"tail\",\n",
    "                \"leftwing\",\n",
    "                \"rightwing\",\n",
    "            ],\n",
    "        }\n",
    "        inference_cfg = {\"topktoretain\": 8, \"pcutoff\": 0.1, \"pafthreshold\": 0.1}\n",
    "        results = crossvalutils._benchmark_paf_graphs(\n",
    "            cfg,\n",
    "            inference_cfg,\n",
    "            data,\n",
    "            [BEST_GRAPH_MONTBLANC],\n",
    "            split_inds=[metadata[\"data\"][\"trainIndices\"], metadata[\"data\"][\"testIndices\"]],\n",
    "        )\n",
    ">       with open(\"tests/data/montblanc_map.pickle\", \"rb\") as file:\n",
    "E       FileNotFoundError: [Errno 2] No such file or directory: 'tests/data/montblanc_map.pickle'\n",
    "\n",
    "test_crossvalutils.py:101: FileNotFoundError\n",
    "------------------------------------------------ Captured stdout call -------------------------------------------------\n",
    "Graph 1|1\n",
    "------------------------------------------------ Captured stderr call -------------------------------------------------\n",
    "30it [00:00, 868.03it/s]\n",
    "100%|██████████| 27/27 [00:00<00:00, 462.92it/s]\n",
    "100%|██████████| 2/2 [00:00<00:00, 456.75it/s]\n",
    "100%|██████████| 30/30 [00:00<00:00, 2535.09it/s]\n",
    "_______________________________________________ test_stitcher_montblanc _______________________________________________\n",
    "\n",
    "real_tracklets_montblanc = {'single': {0: array([[1.344887e+03, 5.726160e+02, 1.000000e+00]]), 1: array([[1.375972e+03, 5.733890e+02, 1.000000e+0...1.   ,  -1.   ],\n",
    "       [684.244, 294.618,   1.   ,  -1.   ],\n",
    "       [668.924, 296.99 ,   1.   ,  -1.   ]]), ...}, ...}\n",
    "\n",
    "    def test_stitcher_montblanc(real_tracklets_montblanc):\n",
    "        stitcher = TrackletStitcher.from_dict_of_dict(\n",
    "            real_tracklets_montblanc,\n",
    "            n_tracks=3,\n",
    "        )\n",
    "        assert len(stitcher) == 5\n",
    "        assert all(tracklet.is_continuous for tracklet in stitcher.tracklets)\n",
    "        assert all(tracklet.identity == -1 for tracklet in stitcher.tracklets)\n",
    "        assert len(stitcher.residuals) == 1\n",
    "        assert len(stitcher.residuals[0]) == 2\n",
    "        assert stitcher.compute_max_gap(stitcher.tracklets) == 5\n",
    "    \n",
    "        stitcher.build_graph()\n",
    "        assert stitcher.G.number_of_edges() == 18\n",
    "        weights = [w for *_, w in stitcher.G.edges.data(\"weight\") if w]\n",
    "        assert weights == [2453, 24498, 5428]\n",
    "    \n",
    "        stitcher.stitch()\n",
    "        assert len(stitcher.tracks) == 3\n",
    "        assert all(len(track) >= 176 for track in stitcher.tracks)\n",
    "        assert all(0.996 <= track.likelihood <= 1 for track in stitcher.tracks)\n",
    "    \n",
    ">       df_gt = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_stitcher.py:226: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "------------------------------------------------ Captured stderr call -------------------------------------------------\n",
    "100%|██████████| 5/5 [00:00<00:00, 9855.04it/s]\n",
    "____________________________________________ test_undistort_views[1-False] ____________________________________________\n",
    "\n",
    "n_view_pairs = 1, is_multi = False\n",
    "stereo_params = {'P1': array([[0.51585028, 0.7419184 , 0.98985395, 0.86273176],\n",
    "       [0.21977008, 0.3603367 , 0.07041644, 0.78424361...       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), 'R2': array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), ...}\n",
    "\n",
    "    @pytest.mark.parametrize(\n",
    "        \"n_view_pairs, is_multi\",\n",
    "        [(i, flag) for i in range(1, 7, 2) for flag in (False, True)],\n",
    "    )\n",
    "    def test_undistort_views(n_view_pairs, is_multi, stereo_params):\n",
    ">       df = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_triangulation.py:46: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "____________________________________________ test_undistort_views[1-True] _____________________________________________\n",
    "\n",
    "n_view_pairs = 1, is_multi = True\n",
    "stereo_params = {'P1': array([[0.51585028, 0.7419184 , 0.98985395, 0.86273176],\n",
    "       [0.21977008, 0.3603367 , 0.07041644, 0.78424361...       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), 'R2': array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), ...}\n",
    "\n",
    "    @pytest.mark.parametrize(\n",
    "        \"n_view_pairs, is_multi\",\n",
    "        [(i, flag) for i in range(1, 7, 2) for flag in (False, True)],\n",
    "    )\n",
    "    def test_undistort_views(n_view_pairs, is_multi, stereo_params):\n",
    ">       df = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_triangulation.py:46: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "____________________________________________ test_undistort_views[3-False] ____________________________________________\n",
    "\n",
    "n_view_pairs = 3, is_multi = False\n",
    "stereo_params = {'P1': array([[0.51585028, 0.7419184 , 0.98985395, 0.86273176],\n",
    "       [0.21977008, 0.3603367 , 0.07041644, 0.78424361...       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), 'R2': array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), ...}\n",
    "\n",
    "    @pytest.mark.parametrize(\n",
    "        \"n_view_pairs, is_multi\",\n",
    "        [(i, flag) for i in range(1, 7, 2) for flag in (False, True)],\n",
    "    )\n",
    "    def test_undistort_views(n_view_pairs, is_multi, stereo_params):\n",
    ">       df = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_triangulation.py:46: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "____________________________________________ test_undistort_views[3-True] _____________________________________________\n",
    "\n",
    "n_view_pairs = 3, is_multi = True\n",
    "stereo_params = {'P1': array([[0.51585028, 0.7419184 , 0.98985395, 0.86273176],\n",
    "       [0.21977008, 0.3603367 , 0.07041644, 0.78424361...       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), 'R2': array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), ...}\n",
    "\n",
    "    @pytest.mark.parametrize(\n",
    "        \"n_view_pairs, is_multi\",\n",
    "        [(i, flag) for i in range(1, 7, 2) for flag in (False, True)],\n",
    "    )\n",
    "    def test_undistort_views(n_view_pairs, is_multi, stereo_params):\n",
    ">       df = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_triangulation.py:46: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "____________________________________________ test_undistort_views[5-False] ____________________________________________\n",
    "\n",
    "n_view_pairs = 5, is_multi = False\n",
    "stereo_params = {'P1': array([[0.51585028, 0.7419184 , 0.98985395, 0.86273176],\n",
    "       [0.21977008, 0.3603367 , 0.07041644, 0.78424361...       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), 'R2': array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), ...}\n",
    "\n",
    "    @pytest.mark.parametrize(\n",
    "        \"n_view_pairs, is_multi\",\n",
    "        [(i, flag) for i in range(1, 7, 2) for flag in (False, True)],\n",
    "    )\n",
    "    def test_undistort_views(n_view_pairs, is_multi, stereo_params):\n",
    ">       df = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_triangulation.py:46: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "____________________________________________ test_undistort_views[5-True] _____________________________________________\n",
    "\n",
    "n_view_pairs = 5, is_multi = True\n",
    "stereo_params = {'P1': array([[0.51585028, 0.7419184 , 0.98985395, 0.86273176],\n",
    "       [0.21977008, 0.3603367 , 0.07041644, 0.78424361...       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), 'R2': array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]]), ...}\n",
    "\n",
    "    @pytest.mark.parametrize(\n",
    "        \"n_view_pairs, is_multi\",\n",
    "        [(i, flag) for i in range(1, 7, 2) for flag in (False, True)],\n",
    "    )\n",
    "    def test_undistort_views(n_view_pairs, is_multi, stereo_params):\n",
    ">       df = pd.read_hdf(\"tests/data/montblanc_tracks.h5\")\n",
    "\n",
    "test_triangulation.py:46: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "path_or_buf = 'tests/data/montblanc_tracks.h5', key = None, mode = 'r', errors = 'strict', where = None, start = None\n",
    "stop = None, columns = None, iterator = False, chunksize = None, kwargs = {}, exists = False\n",
    "\n",
    "    def read_hdf(\n",
    "        path_or_buf: FilePath | HDFStore,\n",
    "        key=None,\n",
    "        mode: str = \"r\",\n",
    "        errors: str = \"strict\",\n",
    "        where: str | list | None = None,\n",
    "        start: int | None = None,\n",
    "        stop: int | None = None,\n",
    "        columns: list[str] | None = None,\n",
    "        iterator: bool = False,\n",
    "        chunksize: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Read from the store, close it if we opened it.\n",
    "    \n",
    "        Retrieve pandas object stored in file, optionally based on where\n",
    "        criteria.\n",
    "    \n",
    "        .. warning::\n",
    "    \n",
    "           Pandas uses PyTables for reading and writing HDF5 files, which allows\n",
    "           serializing object-dtype data with pickle when using the \"fixed\" format.\n",
    "           Loading pickled data received from untrusted sources can be unsafe.\n",
    "    \n",
    "           See: https://docs.python.org/3/library/pickle.html for more.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_or_buf : str, path object, pandas.HDFStore\n",
    "            Any valid string path is acceptable. Only supports the local file system,\n",
    "            remote URLs and file-like objects are not supported.\n",
    "    \n",
    "            If you want to pass in a path object, pandas accepts any\n",
    "            ``os.PathLike``.\n",
    "    \n",
    "            Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.\n",
    "    \n",
    "        key : object, optional\n",
    "            The group identifier in the store. Can be omitted if the HDF file\n",
    "            contains a single pandas object.\n",
    "        mode : {'r', 'r+', 'a'}, default 'r'\n",
    "            Mode to use when opening the file. Ignored if path_or_buf is a\n",
    "            :class:`pandas.HDFStore`. Default is 'r'.\n",
    "        errors : str, default 'strict'\n",
    "            Specifies how encoding and decoding errors are to be handled.\n",
    "            See the errors argument for :func:`open` for a full list\n",
    "            of options.\n",
    "        where : list, optional\n",
    "            A list of Term (or convertible) objects.\n",
    "        start : int, optional\n",
    "            Row number to start selection.\n",
    "        stop  : int, optional\n",
    "            Row number to stop selection.\n",
    "        columns : list, optional\n",
    "            A list of columns names to return.\n",
    "        iterator : bool, optional\n",
    "            Return an iterator object.\n",
    "        chunksize : int, optional\n",
    "            Number of rows to include in an iteration when using an iterator.\n",
    "        **kwargs\n",
    "            Additional keyword arguments passed to HDFStore.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            The selected object. Return type depends on the object stored.\n",
    "    \n",
    "        See Also\n",
    "        --------\n",
    "        DataFrame.to_hdf : Write a HDF file from a DataFrame.\n",
    "        HDFStore : Low-level access to HDF files.\n",
    "    \n",
    "        Examples\n",
    "        --------\n",
    "        >>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP\n",
    "        >>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP\n",
    "        >>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP\n",
    "        \"\"\"\n",
    "        if mode not in [\"r\", \"r+\", \"a\"]:\n",
    "            raise ValueError(\n",
    "                f\"mode {mode} is not allowed while performing a read. \"\n",
    "                f\"Allowed modes are r, r+ and a.\"\n",
    "            )\n",
    "        # grab the scope\n",
    "        if where is not None:\n",
    "            where = _ensure_term(where, scope_level=1)\n",
    "    \n",
    "        if isinstance(path_or_buf, HDFStore):\n",
    "            if not path_or_buf.is_open:\n",
    "                raise OSError(\"The HDFStore must be open for reading.\")\n",
    "    \n",
    "            store = path_or_buf\n",
    "            auto_close = False\n",
    "        else:\n",
    "            path_or_buf = stringify_path(path_or_buf)\n",
    "            if not isinstance(path_or_buf, str):\n",
    "                raise NotImplementedError(\n",
    "                    \"Support for generic buffers has not been implemented.\"\n",
    "                )\n",
    "            try:\n",
    "                exists = os.path.exists(path_or_buf)\n",
    "    \n",
    "            # if filepath is too long\n",
    "            except (TypeError, ValueError):\n",
    "                exists = False\n",
    "    \n",
    "            if not exists:\n",
    ">               raise FileNotFoundError(f\"File {path_or_buf} does not exist\")\n",
    "E               FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pandas/io/pytables.py:418: FileNotFoundError\n",
    "_____________________________________________ test_reader_check_integrity _____________________________________________\n",
    "\n",
    "video_clip = Video (duration=8.53, fps=30.0, dimensions=416x374)\n",
    "\n",
    "    def test_reader_check_integrity(video_clip):\n",
    "        video_clip.check_integrity()\n",
    "        log_file = os.path.join(video_clip.directory, f\"{video_clip.name}.log\")\n",
    ">       assert os.path.getsize(log_file) == 0\n",
    "E       AssertionError: assert 35 == 0\n",
    "E        +  where 35 = <function getsize at 0x7f98ef5f55e0>('/Users/rae/Desktop/DLCdev/tests/data/vid.log')\n",
    "E        +    where <function getsize at 0x7f98ef5f55e0> = <module 'posixpath' from '/usr/local/anaconda3/envs/dlc/lib/python3.9/posixpath.py'>.getsize\n",
    "E        +      where <module 'posixpath' from '/usr/local/anaconda3/envs/dlc/lib/python3.9/posixpath.py'> = os.path\n",
    "\n",
    "test_video.py:37: AssertionError\n",
    "________________________________________________ test_reader_metadata _________________________________________________\n",
    "\n",
    "video_clip = Video (duration=8.53, fps=30.0, dimensions=416x374)\n",
    "\n",
    "    def test_reader_metadata(video_clip):\n",
    "        metadata = video_clip.metadata\n",
    ">       assert metadata[\"n_frames\"] == video_clip.get_n_frames(True) == 256\n",
    "\n",
    "test_video.py:48: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:101: in get_n_frames\n",
    "    output = subprocess.check_output(\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/subprocess.py:424: in check_output\n",
    "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "input = None, capture_output = False, timeout = None, check = True\n",
    "popenargs = ('ffprobe -i \"/Users/rae/Desktop/DLCdev/tests/data/vid.avi\" -v error -count_frames -select_streams v:0 -show_entries stream=nb_read_frames -of default=nokey=1:noprint_wrappers=1',)\n",
    "kwargs = {'shell': True, 'stderr': -2, 'stdout': -1}\n",
    "process = <Popen: returncode: 127 args: 'ffprobe -i \"/Users/rae/Desktop/DLCdev/tests/d...>\n",
    "stdout = b'/bin/sh: ffprobe: command not found\\n', stderr = None, retcode = 127\n",
    "\n",
    "    def run(*popenargs,\n",
    "            input=None, capture_output=False, timeout=None, check=False, **kwargs):\n",
    "        \"\"\"Run command with arguments and return a CompletedProcess instance.\n",
    "    \n",
    "        The returned instance will have attributes args, returncode, stdout and\n",
    "        stderr. By default, stdout and stderr are not captured, and those attributes\n",
    "        will be None. Pass stdout=PIPE and/or stderr=PIPE in order to capture them.\n",
    "    \n",
    "        If check is True and the exit code was non-zero, it raises a\n",
    "        CalledProcessError. The CalledProcessError object will have the return code\n",
    "        in the returncode attribute, and output & stderr attributes if those streams\n",
    "        were captured.\n",
    "    \n",
    "        If timeout is given, and the process takes too long, a TimeoutExpired\n",
    "        exception will be raised.\n",
    "    \n",
    "        There is an optional argument \"input\", allowing you to\n",
    "        pass bytes or a string to the subprocess's stdin.  If you use this argument\n",
    "        you may not also use the Popen constructor's \"stdin\" argument, as\n",
    "        it will be used internally.\n",
    "    \n",
    "        By default, all communication is in bytes, and therefore any \"input\" should\n",
    "        be bytes, and the stdout and stderr will be bytes. If in text mode, any\n",
    "        \"input\" should be a string, and stdout and stderr will be strings decoded\n",
    "        according to locale encoding, or by \"encoding\" if set. Text mode is\n",
    "        triggered by setting any of text, encoding, errors or universal_newlines.\n",
    "    \n",
    "        The other arguments are the same as for the Popen constructor.\n",
    "        \"\"\"\n",
    "        if input is not None:\n",
    "            if kwargs.get('stdin') is not None:\n",
    "                raise ValueError('stdin and input arguments may not both be used.')\n",
    "            kwargs['stdin'] = PIPE\n",
    "    \n",
    "        if capture_output:\n",
    "            if kwargs.get('stdout') is not None or kwargs.get('stderr') is not None:\n",
    "                raise ValueError('stdout and stderr arguments may not be used '\n",
    "                                 'with capture_output.')\n",
    "            kwargs['stdout'] = PIPE\n",
    "            kwargs['stderr'] = PIPE\n",
    "    \n",
    "        with Popen(*popenargs, **kwargs) as process:\n",
    "            try:\n",
    "                stdout, stderr = process.communicate(input, timeout=timeout)\n",
    "            except TimeoutExpired as exc:\n",
    "                process.kill()\n",
    "                if _mswindows:\n",
    "                    # Windows accumulates the output in a single blocking\n",
    "                    # read() call run on child threads, with the timeout\n",
    "                    # being done in a join() on those threads.  communicate()\n",
    "                    # _after_ kill() is required to collect that and add it\n",
    "                    # to the exception.\n",
    "                    exc.stdout, exc.stderr = process.communicate()\n",
    "                else:\n",
    "                    # POSIX _communicate already populated the output so\n",
    "                    # far into the TimeoutExpired exception.\n",
    "                    process.wait()\n",
    "                raise\n",
    "            except:  # Including KeyboardInterrupt, communicate handled that.\n",
    "                process.kill()\n",
    "                # We don't call process.wait() as .__exit__ does that for us.\n",
    "                raise\n",
    "            retcode = process.poll()\n",
    "            if check and retcode:\n",
    ">               raise CalledProcessError(retcode, process.args,\n",
    "                                         output=stdout, stderr=stderr)\n",
    "E               subprocess.CalledProcessError: Command 'ffprobe -i \"/Users/rae/Desktop/DLCdev/tests/data/vid.avi\" -v error -count_frames -select_streams v:0 -show_entries stream=nb_read_frames -of default=nokey=1:noprint_wrappers=1' returned non-zero exit status 127.\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/subprocess.py:528: CalledProcessError\n",
    "_________________________________________________ test_writer_shorten _________________________________________________\n",
    "\n",
    "tmp_path = PosixPath('/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_shorten0')\n",
    "video_clip = Video (duration=8.53, fps=30.0, dimensions=416x374)\n",
    "\n",
    "    def test_writer_shorten(tmp_path, video_clip):\n",
    "        file = video_clip.shorten(\"00:00:00\", \"00:00:02\", dest_folder=str(tmp_path))\n",
    ">       vid = VideoWriter(file)\n",
    "\n",
    "test_video.py:106: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:202: in __init__\n",
    "    super(VideoWriter, self).__init__(video_path)\n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "self = <[AttributeError(\"'VideoWriter' object has no attribute '_n_frames'\") raised in repr()] VideoWriter object at 0x7f98d07a5e50>\n",
    "video_path = '/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_shorten0/vidshort.avi'\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        if not os.path.isfile(video_path):\n",
    ">           raise ValueError(f'Video path \"{video_path}\" does not point to a file.')\n",
    "E           ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_shorten0/vidshort.avi\" does not point to a file.\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:40: ValueError\n",
    "------------------------------------------------ Captured stderr call -------------------------------------------------\n",
    "/bin/sh: ffmpeg: command not found\n",
    "__________________________________________________ test_writer_split __________________________________________________\n",
    "\n",
    "tmp_path = PosixPath('/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_split0')\n",
    "video_clip = Video (duration=8.53, fps=30.0, dimensions=416x374)\n",
    "\n",
    "    def test_writer_split(tmp_path, video_clip):\n",
    "        with pytest.raises(ValueError):\n",
    "            video_clip.split(1)\n",
    "        n_splits = 3\n",
    "        clips = video_clip.split(n_splits, dest_folder=str(tmp_path))\n",
    "        assert len(clips) == n_splits\n",
    ">       vid = VideoWriter(clips[0])\n",
    "\n",
    "test_video.py:116: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:202: in __init__\n",
    "    super(VideoWriter, self).__init__(video_path)\n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "self = <[AttributeError(\"'VideoWriter' object has no attribute '_n_frames'\") raised in repr()] VideoWriter object at 0x7f98d2029f40>\n",
    "video_path = '/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_split0/vidsplit1.avi'\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        if not os.path.isfile(video_path):\n",
    ">           raise ValueError(f'Video path \"{video_path}\" does not point to a file.')\n",
    "E           ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_split0/vidsplit1.avi\" does not point to a file.\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:40: ValueError\n",
    "------------------------------------------------ Captured stderr call -------------------------------------------------\n",
    "/bin/sh: ffmpeg: command not found\n",
    "/bin/sh: ffmpeg: command not found\n",
    "/bin/sh: ffmpeg: command not found\n",
    "__________________________________________________ test_writer_crop ___________________________________________________\n",
    "\n",
    "tmp_path = PosixPath('/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_crop0')\n",
    "video_clip = Video (duration=8.53, fps=30.0, dimensions=50x100)\n",
    "\n",
    "    def test_writer_crop(tmp_path, video_clip):\n",
    "        x1, x2, y1, y2 = 0, 50, 0, 100\n",
    "        video_clip.set_bbox(x1, x2, y1, y2)\n",
    "        file = video_clip.crop(dest_folder=str(tmp_path))\n",
    ">       vid = VideoWriter(file)\n",
    "\n",
    "test_video.py:124: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:202: in __init__\n",
    "    super(VideoWriter, self).__init__(video_path)\n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "self = <[AttributeError(\"'VideoWriter' object has no attribute '_n_frames'\") raised in repr()] VideoWriter object at 0x7f98d1fc2dc0>\n",
    "video_path = '/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_crop0/vidcrop.avi'\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        if not os.path.isfile(video_path):\n",
    ">           raise ValueError(f'Video path \"{video_path}\" does not point to a file.')\n",
    "E           ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_crop0/vidcrop.avi\" does not point to a file.\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:40: ValueError\n",
    "------------------------------------------------ Captured stderr call -------------------------------------------------\n",
    "/bin/sh: ffmpeg: command not found\n",
    "______________________________________________ test_writer_rescale[200] _______________________________________________\n",
    "\n",
    "tmp_path = PosixPath('/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_rescale_200_0')\n",
    "video_clip = Video (duration=8.53, fps=30.0, dimensions=416x374), target_height = 200\n",
    "\n",
    "    @pytest.mark.parametrize(\"target_height\", [200, 177])\n",
    "    def test_writer_rescale(tmp_path, video_clip, target_height):\n",
    "        file = video_clip.rescale(width=-1, height=target_height, dest_folder=str(tmp_path))\n",
    ">       vid = VideoWriter(file)\n",
    "\n",
    "test_video.py:131: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:202: in __init__\n",
    "    super(VideoWriter, self).__init__(video_path)\n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "self = <[AttributeError(\"'VideoWriter' object has no attribute '_n_frames'\") raised in repr()] VideoWriter object at 0x7f98d11724f0>\n",
    "video_path = '/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_rescale_200_0/vidrescale.avi'\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        if not os.path.isfile(video_path):\n",
    ">           raise ValueError(f'Video path \"{video_path}\" does not point to a file.')\n",
    "E           ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_rescale_200_0/vidrescale.avi\" does not point to a file.\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:40: ValueError\n",
    "------------------------------------------------ Captured stderr call -------------------------------------------------\n",
    "/bin/sh: ffmpeg: command not found\n",
    "______________________________________________ test_writer_rescale[177] _______________________________________________\n",
    "\n",
    "tmp_path = PosixPath('/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_rescale_177_0')\n",
    "video_clip = Video (duration=8.53, fps=30.0, dimensions=416x374), target_height = 177\n",
    "\n",
    "    @pytest.mark.parametrize(\"target_height\", [200, 177])\n",
    "    def test_writer_rescale(tmp_path, video_clip, target_height):\n",
    "        file = video_clip.rescale(width=-1, height=target_height, dest_folder=str(tmp_path))\n",
    ">       vid = VideoWriter(file)\n",
    "\n",
    "test_video.py:131: \n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:202: in __init__\n",
    "    super(VideoWriter, self).__init__(video_path)\n",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
    "\n",
    "self = <[AttributeError(\"'VideoWriter' object has no attribute '_n_frames'\") raised in repr()] VideoWriter object at 0x7f98d2143070>\n",
    "video_path = '/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_rescale_177_0/vidrescale.avi'\n",
    "\n",
    "    def __init__(self, video_path):\n",
    "        if not os.path.isfile(video_path):\n",
    ">           raise ValueError(f'Video path \"{video_path}\" does not point to a file.')\n",
    "E           ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_writer_rescale_177_0/vidrescale.avi\" does not point to a file.\n",
    "\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:40: ValueError\n",
    "------------------------------------------------ Captured stderr call -------------------------------------------------\n",
    "/bin/sh: ffmpeg: command not found\n",
    "================================================== warnings summary ===================================================\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "tests/test_auxfun_multianimal.py::test_prune_paf_graph\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_multianimal.py:150: DeprecationWarning: Sampling from a set deprecated\n",
    "  since Python 3.9 and will be removed in a subsequent version.\n",
    "    g = nx.Graph(random.sample(G.edges, desired_n_edges))\n",
    "\n",
    "tests/test_inferenceutils.py::test_find_outlier_assemblies\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/lib/inferenceutils.py:1010: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
    "  Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
    "    lo, up = np.percentile(vals, qs, interpolation=\"nearest\")\n",
    "\n",
    "tests/test_predict_multianimal.py: 12 warnings\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict_multianimal.py:268: DeprecationWarning: Please use `label` from the `scipy.ndimage` namespace, the `scipy.ndimage.measurements` namespace is deprecated.\n",
    "    labels = measurements.label(grid)[0]\n",
    "\n",
    "tests/test_predict_multianimal.py: 12 warnings\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict_multianimal.py:269: DeprecationWarning: Please use `center_of_mass` from the `scipy.ndimage` namespace, the `scipy.ndimage.measurements` namespace is deprecated.\n",
    "    xy = measurements.center_of_mass(grid, labels, range(1, np.max(labels) + 1))\n",
    "\n",
    "tests/test_predict_multianimal.py: 12 warnings\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/core/predict_multianimal.py:270: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
    "  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
    "    return np.asarray(xy, dtype=np.int).reshape((-1, 2))\n",
    "\n",
    "tests/test_stitcher.py::test_tracklet[tracklet0]\n",
    "tests/test_stitcher.py::test_tracklet[tracklet1]\n",
    "tests/test_stitcher.py::test_stitcher_real\n",
    "tests/test_stitcher.py::test_stitcher_montblanc\n",
    "tests/test_stitcher.py::test_stitcher_with_identity\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/refine_training_dataset/stitch.py:138: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
    "    return mode(self.data[..., 3], axis=None, nan_policy=\"omit\")[0][0]\n",
    "\n",
    "tests/test_stitcher.py::test_tracklet_affinities[tracklet1]\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/refine_training_dataset/stitch.py:416: RuntimeWarning: invalid value encountered in divide\n",
    "    diff = np.abs(np.diff(eigen / eigen[0]))\n",
    "\n",
    "tests/test_stitcher.py::test_tracklet_affinities[tracklet1]\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/refine_training_dataset/stitch.py:399: RuntimeWarning: invalid value encountered in long_scalars\n",
    "    return (rank1 + rank2) / joint_rank - 1\n",
    "\n",
    "tests/test_stitcher.py::test_tracklet_affinities[tracklet1]\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/refine_training_dataset/stitch.py:374: RuntimeWarning: invalid value encountered in divide\n",
    "    hk1 /= np.linalg.norm(hk1)\n",
    "\n",
    "tests/test_stitcher.py::test_tracklet_affinities[tracklet1]\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/refine_training_dataset/stitch.py:376: RuntimeWarning: invalid value encountered in divide\n",
    "    hk2 /= np.linalg.norm(hk2)\n",
    "\n",
    "tests/test_stitcher.py::test_tracklet_affinities[tracklet1]\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py:45: RuntimeWarning: invalid value encountered in double_scalars\n",
    "    return wh / (\n",
    "\n",
    "tests/test_trackingutils.py::test_ellipse_fitter\n",
    "  /Users/rae/Desktop/DLCdev/tests/test_trackingutils.py:36: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
    "  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
    "    xy = np.asarray([[-2, 0], [2, 0], [0, 1], [0, -1]], dtype=np.float)\n",
    "\n",
    "tests/test_trackingutils.py::test_ellipse_fitter\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py:220: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (Array(float64, 2, 'C', False, aligned=True), Array(float64, 1, 'A', False, aligned=True))\n",
    "    a2 = T @ a1\n",
    "\n",
    "tests/test_trackingutils.py: 836 warnings\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py:761: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
    "  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
    "    tracklet_id, pred_id = content[-2:].astype(np.int)\n",
    "\n",
    "tests/test_trainingsetmanipulation.py::test_read_image_shape_fast\n",
    "  /Users/rae/Desktop/DLCdev/tests/test_trainingsetmanipulation.py:35: UserWarning: /private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_read_image_shape_fast0/gray.png is a low contrast image\n",
    "    io.imsave(path_gray_image, color.rgb2gray(img).astype(np.uint8))\n",
    "\n",
    "tests/test_video.py::test_reader_check_integrity\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:61: UserWarning: Video contains errors. See \"/Users/rae/Desktop/DLCdev/tests/data/vid.log\" for a detailed report.\n",
    "    warnings.warn(f'Video contains errors. See \"{dest}\" for a detailed report.')\n",
    "\n",
    "tests/test_video.py::test_reader_set_frame\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:124: UserWarning: Index exceeds the total number of frames. Setting to last frame instead.\n",
    "    warnings.warn(\n",
    "\n",
    "tests/test_video.py::test_writer_bbox\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/utils/auxfun_videos.py:220: UserWarning: Bounding box larger than the video... Clipping to video dimensions.\n",
    "    warnings.warn(\n",
    "\n",
    "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
    "=============================================== short test summary info ===============================================\n",
    "FAILED test_auxfun_multianimal.py::test_reorder_individuals_in_df - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_crossvalutils.py::test_benchmark_paf_graphs_montblanc - FileNotFoundError: [Errno 2] No such file or directory: 'tests/data/montblanc_map.pickle'\n",
    "FAILED test_stitcher.py::test_stitcher_montblanc - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_triangulation.py::test_undistort_views[1-False] - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_triangulation.py::test_undistort_views[1-True] - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_triangulation.py::test_undistort_views[3-False] - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_triangulation.py::test_undistort_views[3-True] - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_triangulation.py::test_undistort_views[5-False] - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_triangulation.py::test_undistort_views[5-True] - FileNotFoundError: File tests/data/montblanc_tracks.h5 does not exist\n",
    "FAILED test_video.py::test_reader_check_integrity - AssertionError: assert 35 == 0\n",
    "FAILED test_video.py::test_reader_metadata - subprocess.CalledProcessError: Command 'ffprobe -i \"/Users/rae/Desktop/DLCdev/tests/data/vid.avi\" -v error -count_...\n",
    "FAILED test_video.py::test_writer_shorten - ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_write...\n",
    "FAILED test_video.py::test_writer_split - ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_write...\n",
    "FAILED test_video.py::test_writer_crop - ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_write...\n",
    "FAILED test_video.py::test_writer_rescale[200] - ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_write...\n",
    "FAILED test_video.py::test_writer_rescale[177] - ValueError: Video path \"/private/var/folders/ct/yqddf6yn3fb26n4v2bp3mxxw0000gq/T/pytest-of-rae/pytest-0/test_write...\n",
    "==================================== 16 failed, 99 passed, 897 warnings in 16.59s =====================================\n",
    "(dlc) raeさんは testsに >> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e13f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from deeplabcut.pose_estimation_pytorch.solvers import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a847c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m pytest test_pose_estimation_pytorch_solvers_utils.py                                 と言います!★ \n",
    "Extracting: 100%|███████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 685.91it/s]\n",
    "========================================================== test session starts ==========================================================\n",
    "platform darwin -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0\n",
    "rootdir: /Users/rae/Desktop/DLCdev\n",
    "plugins: npe2-0.7.0, napari-0.4.17, napari-plugin-engine-0.2.0\n",
    "collected 0 items / 1 error                                                                                                             \n",
    "\n",
    "================================================================ ERRORS =================================================================\n",
    "_________________________________ ERROR collecting tests/test_pose_estimation_pytorch_solvers_utils.py __________________________________\n",
    "ImportError while importing test module '/Users/rae/Desktop/DLCdev/tests/test_pose_estimation_pytorch_solvers_utils.py'.\n",
    "Hint: make sure your test modules/packages have valid Python names.\n",
    "Traceback:\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/importlib/__init__.py:127: in import_module\n",
    "    return _bootstrap._gcd_import(name[level:], package, level)\n",
    "test_pose_estimation_pytorch_solvers_utils.py:3: in <module>\n",
    "    from deeplabcut.pose_estimation_pytorch.solvers import utils\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_pytorch/__init__.py:4: in <module>\n",
    "    from deeplabcut.pose_estimation_pytorch.apis import (\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_pytorch/apis/__init__.py:12: in <module>\n",
    "    from deeplabcut.pose_estimation_pytorch.apis.analyze_videos import analyze_videos\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_pytorch/apis/analyze_videos.py:29: in <module>\n",
    "    from deeplabcut.pose_estimation_pytorch.apis.utils import (\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_pytorch/apis/utils.py:10: in <module>\n",
    "    from deeplabcut.pose_estimation_pytorch.solvers import LOGGER, SINGLE_ANIMAL_SOLVER\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_pytorch/solvers/__init__.py:1: in <module>\n",
    "    from deeplabcut.pose_estimation_pytorch.solvers.logger import LOGGER\n",
    "/usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/deeplabcut/pose_estimation_pytorch/solvers/logger.py:3: in <module>\n",
    "    import wandb as wb\n",
    "E   ModuleNotFoundError: No module named 'wandb'\n",
    "------------------------------------------------------------ Captured stdout ------------------------------------------------------------\n",
    "Loading DLC 2.3.1...\n",
    "------------------------------------------------------------ Captured stderr ------------------------------------------------------------\n",
    "2023-06-27 16:55:23.221777: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "======================================================== short test summary info ========================================================\n",
    "ERROR test_pose_estimation_pytorch_solvers_utils.py\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "=========================================================== 1 error in 2.41s ======================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf41233",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dlc) raeさんは testsに >> pip install wandb                                                                               と言います!★ \n",
    "Collecting wandb\n",
    "  Using cached wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
    "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (8.1.3)\n",
    "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
    "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
    "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
    "Requirement already satisfied: psutil>=5.0.0 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (5.9.5)\n",
    "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
    "  Downloading sentry_sdk-1.26.0-py2.py3-none-any.whl (209 kB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.4/209.4 kB 3.0 MB/s eta 0:00:00\n",
    "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
    "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
    "Requirement already satisfied: PyYAML in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (6.0)\n",
    "Collecting pathtools (from wandb)\n",
    "  Using cached pathtools-0.1.2.tar.gz (11 kB)\n",
    "  Preparing metadata (setup.py) ... done\n",
    "Collecting setproctitle (from wandb)\n",
    "  Downloading setproctitle-1.3.2-cp39-cp39-macosx_10_9_x86_64.whl (11 kB)\n",
    "Requirement already satisfied: setuptools in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (67.8.0)\n",
    "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
    "Requirement already satisfied: typing-extensions in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (4.6.3)\n",
    "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from wandb) (4.23.3)\n",
    "Requirement already satisfied: six>=1.4.0 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
    "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
    "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
    "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
    "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
    "Building wheels for collected packages: pathtools\n",
    "  Building wheel for pathtools (setup.py) ... done\n",
    "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ecde11b01b1a339af72c07fda83bf86585f080e25338399ce632a2ed64a1feef\n",
    "  Stored in directory: /Users/rae/Library/Caches/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
    "Successfully built pathtools\n",
    "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
    "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.26.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.4\n",
    "(dlc) raeさんは testsに >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ddf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    " python3 -m pytest test_pose_estimation_pytorch_solvers_utils.py                                 と言います!★ \n",
    "Extracting: 100%|███████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 718.48it/s]\n",
    "========================================================== test session starts ==========================================================\n",
    "platform darwin -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0\n",
    "rootdir: /Users/rae/Desktop/DLCdev\n",
    "plugins: npe2-0.7.0, napari-0.4.17, napari-plugin-engine-0.2.0\n",
    "collected 0 items                                                                                                                       \n",
    "\n",
    "=========================================================== warnings summary ============================================================\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:121\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
    "    warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel.yaml')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2349\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(parent)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from deeplabcut.pose_estimation_pytorch.solvers import utils\n",
    "import os\n",
    "from deeplabcut.utils import auxiliaryfunctions\n",
    "from deeplabcut.utils.auxfun_videos import SUPPORTED_VIDEOS\n",
    "\n",
    "\n",
    "def test_get_paths(tmpdir_factory):\n",
    "    fake_folder = tmpdir_factory.mktemp(\"videos\")\n",
    "    n_ext = len(SUPPORTED_VIDEOS)\n",
    "\n",
    "    def _create_fake_file(filename):\n",
    "        path = str(fake_folder.join(filename))\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        return path\n",
    "\n",
    "    fake_videos = []\n",
    "    for ext in SUPPORTED_VIDEOS:\n",
    "        path = _create_fake_file(f\"fake.{ext}\")\n",
    "        fake_videos.append(path)\n",
    "\n",
    "    # Add some other office files:\n",
    "    path = _create_fake_file(\"fake.xls\")\n",
    "    path = _create_fake_file(\"fake.pptx\")\n",
    "\n",
    "    # Add a .pickle and .h5 files\n",
    "    _ = _create_fake_file(\"fake.pickle\")\n",
    "    _ = _create_fake_file(\"fake.h5\")\n",
    "\n",
    "    # By default, all videos with common extensions are taken from a directory\n",
    "    videos = auxiliaryfunctions.get_list_of_videos(\n",
    "        str(fake_folder),\n",
    "        videotype=\"\",\n",
    "    )\n",
    "    assert len(videos) == n_ext\n",
    "\n",
    "    # A list of extensions can also be passed in\n",
    "    videos = auxiliaryfunctions.get_list_of_videos(\n",
    "        str(fake_folder),\n",
    "        videotype=SUPPORTED_VIDEOS,\n",
    "    )\n",
    "    assert len(videos) == n_ext\n",
    "\n",
    "    for ext in SUPPORTED_VIDEOS:\n",
    "        videos = auxiliaryfunctions.get_list_of_videos(\n",
    "            str(fake_folder),\n",
    "            videotype=ext,\n",
    "        )\n",
    "        assert len(videos) == 1\n",
    "\n",
    "    videos = auxiliaryfunctions.get_list_of_videos(\n",
    "        str(fake_folder),\n",
    "        videotype=\"unknown\",\n",
    "    )\n",
    "    assert not len(videos)\n",
    "\n",
    "    videos = auxiliaryfunctions.get_list_of_videos(\n",
    "        fake_videos,\n",
    "        videotype=\"\",\n",
    "    )\n",
    "    assert len(videos) == n_ext\n",
    "\n",
    "    for video in fake_videos:\n",
    "        videos = auxiliaryfunctions.get_list_of_videos([video], videotype=\"\")\n",
    "        assert len(videos) == 1\n",
    "\n",
    "    for ext in SUPPORTED_VIDEOS:\n",
    "        videos = auxiliaryfunctions.get_list_of_videos(\n",
    "            fake_videos,\n",
    "            videotype=ext,\n",
    "        )\n",
    "        assert len(videos) == 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dlc) raeさんは testsに >> python3 -m pytest test_pose_estimation_pytorch_solvers_utils.py                                 と言います!★ \n",
    "Extracting: 100%|███████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 630.54it/s]\n",
    "========================================================== test session starts ==========================================================\n",
    "platform darwin -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0\n",
    "rootdir: /Users/rae/Desktop/DLCdev\n",
    "plugins: npe2-0.7.0, napari-0.4.17, napari-plugin-engine-0.2.0\n",
    "collected 1 item                                                                                                                        \n",
    "\n",
    "test_pose_estimation_pytorch_solvers_utils.py .                                                                                   [100%]\n",
    "\n",
    "=========================================================== warnings summary ============================================================\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:121\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
    "    warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel.yaml')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2349\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(parent)\n",
    "\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "../../../../../usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870\n",
    "  /usr/local/anaconda3/envs/dlc/lib/python3.9/site-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
    "  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
    "    declare_namespace(pkg)\n",
    "\n",
    "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
    "==================================================== 1 passed, 10 warnings in 1.23s ====================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
